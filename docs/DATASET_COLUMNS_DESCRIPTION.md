## Understanding Dataset Columns

This section describes the meaning of each column in all dataset subsets that are generated by YourBench.


### Understand the `ingested` subset
<details>
<summary>See more</summary>

The ingested dataset contains the following columns:
- `document_id`: A unique identifier for each file we ingested
- `document_filename`: The name of the file
- `document_text`: The whole text content parsed from the file
- `document_metadata`: Metadata about the file such as the file size.

If you ingest just 1 document, there is just one row in this dataset.

</details>

### Understand the `summarized` subset
<details>
<summary>See more</summary>

The summarized dataset contains the same columns as the `ingested` dataset, plus the following:
- `document_summary`: An LLM-generated summary of the document
- `summarization_model`: The model used to generate the summary

If you ingest just 1 document, there is just one row.

</details>

### Understand the `chunked` subset
<details>
<summary>See more</summary>

The chunked dataset contains the same columns as the `ingested` dataset, plus the following:
- `chunks`: All the chunks of the document (one row corresponds to one document). The chunks are stored as a list of dictionaries with the following keys:
    - `chunk_id`: The ID of the chunk. This ID reuses the document_id and increments a suffix to make it unique. The first chunk has chunk_id of `document_id_0`, the second chunk has chunk_id of `document_id_1`, and so on.
    - `chunk_text`: The actual text content of the chunk, which is used to generate synthetic questions and answer pairs.
- `multihop_chunks`: These are combinations of chunks for multi-hop question generation pipelines. This is a list of dictionaries with the following keys:
    - `chunk_ids`: A list of chunk IDs
    - `chunks_text`: A list of the chunk texts

The ingested documents are split into chunks of tokens with some overlap between tokens. These are parameterized in the `chunking` stage of the pipeline with `l_max_tokens` and `token_overlap`.

The number of rows in the `chunked` subset is equal to the number of ingested documents.

Even if you don't use the `multi_hop_questions` stage, the `chunked` dataset is still useful as it contains the chunks of the documents that are used to generate the questions.

</details>

### Understand the `single_shot_questions` subset
<details>
<summary>See more</summary>

The single_shot_questions dataset contains the same columns as the `chunked` dataset, plus the following:
- `document_id`: The ID of the document that the question was generated from
- `additional_instructions`: The additional instructions that were given to the LLM for single-hop question generation (if any, otherwise it's empty)
- `question`: The single-hop question generated, a question that can be answered using only the content of the chunk. For instance: *What is Professor Snape's true intention during Harry's first Quidditch match?*
- `choices`: (only if `question_mode: multi-choice`, there are no choices for open-ended questions) The choices generated for the question. For instance: ['(A) He is trying to protect Harry by jinxing his broom.', '(B) He wants to sabotage Harry so Gryffindor will lose.', '(C) He is unaware of the match and does nothing.', "(D) He is testing Harry's flying skills."] 
- `self_answer`: The expected correct answer, generated by the LLM when generating the question. For multi-choice questions, it is the letter corresponding to the correct answer, for instance: "A". For open-ended questions, it is the plain text answer itself.
- `chunk_id`: The ID of the chunk used to generate the question. You can find the chunk text in the `chunked` subset.

- `estimated_difficulty`: The difficulty estimated by the LLM when generating the question
- `self_assessed_question_type`: The type of question attributed by the LLM when generating the question
- `generating_model`: The LLM model used to generate the question
- `thought_process`: The thought process of the LLM when generating the question
- `raw_response`: The raw response of the LLM when generating the question. This includes all of the analysis of the provided text and the generated questions formatted as JSON. Can be useful for debugging or for further analysis. The individual questions, thought process, etc. are parsed from this `raw_response` column.
- `citations`: A list of verbatim quotes taken from the source chunk(s) that substantiate the question and its answer. The generation prompt instructs the model to return citations as `List[str]` containing exact excerpts from `<text_chunk>`. Citations are used for:
  - **Grounding**: Anchor each question in the provided document text; they are also used by the optional `citation_score_filtering` stage (fuzzy matching) to flag ungrounded items.
  - **Verification**: Let you confirm the question and answer truly correspond to the source material.
  - **Transparency**: Show precisely which passages the question is based on.

The following columns are present but empty. These are placeholder columns for the question_rewriting stage.
- `original_question`: None
- `question_rewriting_model`: None
- `question_rewriting_rationale`: None
- `raw_question_rewriting_response`: None

The number of rows in the `single_shot_questions` subset depends on the number of questions generated from all chunks across all documents.

</details>

### Understand the `multi_hop_questions` subset
<details>
<summary>See more</summary>

The multi_hop_questions dataset contains the same base columns as the `single_shot_questions` dataset, but with some key differences:

- `question`: The multi-hop question generated, a question that requires reasoning across multiple chunks within the same document. For instance: *"How does Professor Snape's behavior during Harry's first Quidditch match relate to his later actions in protecting Harry throughout the series?"*

- `source_chunk_ids`: A list of chunk IDs from the same document that were used to generate this multi-hop question. Unlike single-shot questions that use a single `chunk_id`, multi-hop questions use multiple chunks from the *chunked* subset to create questions requiring reasoning across different parts of the same document.

The multi-hop questions are generated from the `multihop_chunks` field in the `chunked` dataset, which contains combinations of chunks designed for multi-hop reasoning within individual documents.

</details>

### Understand the `cross_document_questions` subset
<details>
<summary>See more</summary>

The `cross_document_questions` dataset contains the same columns as the `multi_hop_questions` dataset.

The main distinction is in the scope: while `multi_hop_questions` use chunks from the same document, `cross_document_questions` use chunks from multiple different documents. The cross-document dataset is created by combining chunks from different documents according to the configuration parameters like `max_combinations`, `chunks_per_document`, and `num_docs_per_combination`.

Cross-document questions test the ability to reason and make connections across entirely separate documents, making them more challenging than both single-shot and multi-hop questions.

</details>

### Understand the `prepared_lighteval` subset
<details>
<summary>See more</summary>

The `prepared_lighteval` dataset combines questions from all generation stages (`single_shot_questions`, `multi_hop_questions`, `cross_document_questions`) into a unified evaluation format.

The `prepared_lighteval` dataset contains the following columns:

**Core Question Information:**
- `question`: The question text
- `additional_instructions`: Additional context or instructions for the question
- `ground_truth_answer`: The expected correct answer (mapped from `self_answer` in source subsets)
- `gold`: The answer formatted for evaluation frameworks. For multi-choice questions, this is the letter choice converted to a zero-indexed number (e.g., "A" becomes `[0]`). For open-ended questions, this contains the text answer as `[answer_text]`
- `choices`: Available answer choices for multi-choice questions (directly from `choices` in source subsets)

**Question Metadata:**
- `question_category`: The type/category of question (mapped from `self_assessed_question_type` in source subsets)
- `kind`: Identifies the question type as one of: `"single_shot"`, `"multi_hop"`, or `"cross_document"`
- `estimated_difficulty`: Difficulty level on a 1-10 scale
- `question_generating_model`: The model used to generate the question

**Source Traceability:**
- `document_id`: The ID of the primary document the question originates from (mapped from `document_id` in source subsets)
- `chunk_ids`: List of chunk IDs used to generate the question. For single-shot questions, this contains one chunk ID (from `chunk_id`). For multi-hop and cross-document questions, this contains multiple chunk IDs (from `source_chunk_ids`)
- `citations`: List of exact text excerpts that support the question and answer
- `chunks`: List of actual chunk texts corresponding to the `chunk_ids`. This is populated by looking up chunk texts from the `chunked` subset using the chunk IDs
- `document`: The full text of the primary document
- `document_summary`: The summary of the primary document

**Key Features:**
- **Unified Schema**: All question types (single-shot, multi-hop, cross-document) are standardized into the same column structure
- **Rich Context**: Each question includes not just the chunk text, but also the full document and summary for comprehensive context
- **Evaluation Ready**: The `gold` column is formatted for compatibility with evaluation frameworks like LightEval
- **Full Traceability**: Complete mapping back to source chunks and documents for verification and analysis

This dataset serves as the final, evaluation-ready output that combines the best of all question generation stages while maintaining full traceability to the original source material.

After running the `citation_score_filtering` pipeline stage, three additional columns are added to the dataset:
- `answer_citation_score`: Average similarity of ground truth answer to citations
- `chunk_citation_score`: Average similarity of chunks to citations
- `citation_score`: Measures how well the generated answer is grounded in the source material. It is a weighted final score combining the above metrics (default is `final_score = alpha * avg_chunk_score + beta * avg_answer_score`, where `alpha=0.7` and `beta=0.3`)

</details>