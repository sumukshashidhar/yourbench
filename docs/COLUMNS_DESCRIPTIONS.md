## Understand Datasets Columns

This section describes the meaning of each column in all datasets subsets that are generated by YourBench.

### Table of contents
- [Understand the `ingested` subset](#understand-the-ingested-subset)
- [Understand the `summarized` subset](#understand-the-summarized-subset)
- [Understand the `chunked` subset](#understand-the-chunked-subset)
- [Understand the `single_shot_questions` subset](#understand-the-single_shot_questions-subset)

### Understand the `ingested` subset

The ingested dataset contains the following columns:
- `document_id`: A unique identifier for each file we ingested
- `document_filename`: The name of the file
- `document_text`: The whole text content parsed from the file
- `document_metadata`: Metadata about the file such as the file size.

If you ingest just 1 document, there is just one row in this dataset.

### Understand the `summarized` subset

The summarized dataset contains the same columns as the `ingested` dataset, plus the following:
- `document_summary`: An LLM-generated summary of the document
- `summarization_model`: The model used to generate the summary

If you ingest just 1 document, there is just one row.

### Understand the `chunked` subset

The chunked dataset contains the same columns as the `ingested` dataset, plus the two:
- `chunks`: All the chunks of the document (one row correspond to one document). The chunks are stored as a list of dictionaries with the following keys:
    - `chunk_id`: The ID of the chunk. ThÂ§is ID reuses the document_id and increments a suffix to make it unique. The first chunk has chunk_id of *document_id_0*, the second chunk has chunk_id of *document_id_1*, and so on.
    - `chunk_text`: The actual text content of the chunk, which is used to generate synthetic questions and answer pairs.
- `multihop_chunks`: these are combinations of chunks for multi-hop question generation pipelines. This is a list of dictionaries with the following keys:
    - `chunk_ids`: A list of chunk IDs
    - `chunks_text`: A list of the chunk texts

The ingested documents are split into chunks of tokens with some overlap of tokens. These are parametrized in the `chunking` stage of the pipeline with `l_max_tokens` and `token_overlap`.

The number of rows in the `chunked` subset is equal to the number of ingested documents.

Even if you don't use the `multi_hop_questions` stage, the `chunked` dataset is still useful as it contains the chunks of the documents that are used to generate the questions.

### Understand the `single_shot_questions` subset

The single_shot_questions dataset contains the same columns as the `chunked` dataset, plus the following:
- `document_id`: The ID of the document that the question was generated from
- `additional_instructions`: The additional instructions that were given to the LLM for single-hop question generation (if any, otherwise it's empty)
- `question`: The single-hop question generated, a question that can be answered using only the content of the chunk. For instance: *What is Professor Snape's true intention during Harry's first Quidditch match?*
- `choices`: The choices generated for the question. For instance: ['(A) He is trying to protect Harry by jinxing his broom.', '(B) He wants to sabotage Harry so Gryffindor will lose.', '(C) He is unaware of the match and does nothing.', "(D) He is testing Harry's flying skills."] 
- `self_answer`: The letter corresponding to the correct answer, for instance: "A".
- `chunk_id`: The id of the chunk used to generate the question. We are able to find the chunk text in the *chunked* subset.

- `estimated_difficulty`: The difficulty estimated by the LLM when generating the question
- `self_assessed_question_type`: The type of question attributed by the LLM when generating the question
- `generating_model`: The LLM model used to generate the question
- `thought_process`: The thought process of the LLM when generating the question
- `raw_response`: The raw response of the LLM when generating the question. This includes all of the analysis of the provided text and the generates questions formatted as JSON. Can be useful for debugging or for further analysis. The individual questions, thought process etc. are parse from this `raw_response` column.
- `citations`: A list of verbatim quotes taken from the source chunk(s) that substantiate the question and its answer. The generation prompt instructs the model to return citations as `List[str]` containing exact excerpts from `<text_chunk>`. Citations are used for:
  - Grounding: Anchor each question in the provided document text; they are also used by the optional `citation_score_filtering` stage (fuzzy matching) to flag ungrounded items.
  - Verification: Let you confirm the question and answer truly correspond to the source material.
  - Transparency: Show precisely which passages the question is based on.

The following columns are present but empty. There are placeholder columns for the question_rewriting stage.
- `original_question`: None
- `question_rewriting_model`: None
- `question_rewriting_rationale`: None
- `raw_question_rewriting_response`: None
