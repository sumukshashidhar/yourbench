###
# this is a test config file for yourbench, which will help you understand how the yb architecture works.
# you will need a simple openrouter (https://openrouter.ai/) api key, with gemini access to test this.
# this is free!
###
hf_configuration:
  token: $HF_TOKEN
  private: true # true by default, set to false to make the dataset public
  hf_organization: $HF_ORGANIZATION

model_list:
  - model_name: google/gemini-2.0-flash-lite-preview-02-05:free
    provider: openrouter
    base_url: https://openrouter.ai/api/v1
    api_key: $OPENROUTER_API_KEY
    max_concurrent_requests: 1
  - model_name: qwen/qwen-vl-plus:free
    provider: openrouter
    base_url: https://openrouter.ai/api/v1
    api_key: $OPENROUTER_API_KEY
    max_concurrent_requests: 1
  - model_name: cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic
    provider: openai
    base_url: $LOCALHOST_BASE_URL
    api_key: $OPENROUTER_API_KEY
    max_concurrent_requests: 1
  - model_name: mistralai/Mistral-Small-24B-Instruct-2501
    provider: openai
    base_url: $LOCALHOST_BASE_URL_2
    api_key: $OPENROUTER_API_KEY
    max_concurrent_requests: 1

model_roles:
  ingestion:
    # - google/gemini-2.0-flash-lite-preview-02-05:free
    - cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic
  summarization:
    # - google/gemini-2.0-flash-lite-preview-02-05:free
    - cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic
  single_shot_question_generation:
    - cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic
    - mistralai/Mistral-Small-24B-Instruct-2501
  multi_hop_question_generation:
    - cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic
    - mistralai/Mistral-Small-24B-Instruct-2501
    # - qwen/qwen-vl-plus:free

inference_config:
  max_concurrent_requests: 16

# this is the pipeline for yourbench. you can run the stages of the pipeline you wish to.
# the design principle dictates that each stage is self contained, and does not require any existing state from the previous stages
pipeline:
  # ingestion to convert documents to markdown
  ingestion:
    source_documents_dir: data/example/raw
    output_dir: data/example/ingested
    run: false # set to true to run the stage
  
  # upload documents to a simple HF hub repo
  upload_ingest_to_hub:
    source_documents_dir: data/example/ingested
    hub_dataset_name: yb_demo_ingested_documents
    local_dataset_path: data/example/ingested_dataset
    run: false
  
  # summarization to convert markdown to a summary
  summarization:
    source_dataset_name: yb_demo_ingested_documents
    output_dataset_name: yb_demo_ingested_documents_with_summaries
    local_dataset_path: data/example/ingested_dataset_with_summaries
    concat_existing_dataset: false
    run: false

  # chunking to convert document to chunks
  chunking:
    source_dataset_name: yb_demo_ingested_documents_with_summaries
    output_dataset_name: yb_demo_chunked_documents
    local_dataset_path: data/example/chunked_dataset
    concat_existing_dataset: false
    chunking_configuration:
      l_min_tokens: 64
      l_max_tokens: 128
      tau_threshold: 0.3
      h_min: 2
      h_max: 4
    run: false
  
  single_shot_question_generation:
    source_dataset_name: yb_demo_chunked_documents
    output_dataset_name: yb_demo_single_shot_questions
    local_dataset_path: data/example/single_shot_questions
    diversification_seed: "24 year old adult"
    concat_existing_dataset: false
    run: false
  
  multi_hop_question_generation:
    source_dataset_name: yb_demo_chunked_documents
    output_dataset_name: yb_demo_multi_hop_questions
    local_dataset_path: data/example/multi_hop_questions
    concat_existing_dataset: false
    run: true
